{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin4Build: Neural policy control example\n",
    "\n",
    "Welcome to this tutorial on using the Twin4Build package! In this notebook, you'll learn how to:\n",
    "1. Load a simple t4b model\n",
    "2. Define a neural network policy\n",
    "3. Modify the model to incorporate the policy as a controller\n",
    "4. Run a simulation with the neural policy controller\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "First, let's install and import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/JBjoernskov/Twin4Build.git # Uncomment in google colab\n",
    "import twin4build as tb\n",
    "import datetime\n",
    "import twin4build.examples.utils as utils\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "from dateutil.tz import gettz \n",
    "import twin4build.utils.plot.plot as plot\n",
    "import twin4build.utils.types as tps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the Model\n",
    "\n",
    "In this example, we will use the one_room_example_model.xlsm file. \n",
    "This file contains a model of a single room with a space heater and supply and return ventilation.\n",
    "We use the translation engine to automatically create the components and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new model\n",
    "model = tb.Model(id=\"neural_policy_example\")\n",
    "filename = utils.get_path([\"parameter_estimation_example\", \"one_room_example_model.xlsm\"])\n",
    "model.load(semantic_model_filename=filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model validation above we see that different types of information is missing. \n",
    "\n",
    "We will add the missing information as per the parameter estimation example, here we will assume we know the right parameters for this model.\n",
    "\n",
    "1. A method to inject the neural policy (For now with random weights)\n",
    "2. A fcn function which adds the missing connections and schedules as well as injecting the neural policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Incorporating the neural policy controller\n",
    "\n",
    "Now that our model is set up, let's define a control strategy that might benefit from using a neural policy instead of classic controllers. The inputs of the policy could be:\n",
    "- Outdoor temperature\n",
    "- Supply air temperature\n",
    "- Supply water temperature\n",
    "- Space heater valve position\n",
    "- Room supply damper position (In this case, supply and damper position are the same, so one signal provides enough information)\n",
    "- Room CO2 level\n",
    "\n",
    "The outputs would be:\n",
    "\n",
    "- Temperature heating setpoint\n",
    "- CO2 setpoint\n",
    "\n",
    "We will use a policy previously trained with Reinforcement Learning to reduce energy usage in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Controllers Networks:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_bound):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.action_bound = action_bound\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean = nn.Linear(64, action_dim)\n",
    "        self.log_std = tps.Parameter(torch.zeros(action_dim))\n",
    "        # Could add bounds to prevent too small/large standard deviations\n",
    "        self.log_std = tps.Parameter(torch.clamp(self.log_std, min=-20, max=2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        mean = self.mean(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        return mean, std\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Add final layer to output a scalar value\n",
    "        self.value_head = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        value = self.value_head(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_neural_policy_in_fcn(self:tb.Model, input_output_dictionary, policy_path=None):\n",
    "        \"\"\"\n",
    "        The input/output dictionary contains information on the input and output signals of the controller.\n",
    "        These signals must match the component and signal keys to replace in the model\n",
    "        The input dictionary will have items like this:\n",
    "            \"component_key\": {\n",
    "                \"component_output_signal_key\": {\n",
    "                    \"min\": 0,\n",
    "                    \"max\": 1,\n",
    "                    \"description\": \"Description of the signal\"\n",
    "                }\n",
    "            }\n",
    "        Whilst the output items will have a similar structure but for the output signals:\n",
    "            \"component_key\": {\n",
    "                \"component_input_signal_key\": {\n",
    "                    \"min\": 0,\n",
    "                    \"max\": 1,\n",
    "                    \"description\": \"Description of the signal\"\n",
    "                }\n",
    "            }\n",
    "        Note that the input signals must contain the key for the output compoenent signal and the output signals must contain the key for the input component signal\n",
    "\n",
    "        This function instantiates the controller and adds it to the model.\n",
    "        Then it goes through the input dictionary adding connection to the input signals\n",
    "        Then it goes through the output dictionary finding the corresponding existing connections, deleting the existing connections and adding the new connections\n",
    "        \"\"\"\n",
    "        try:\n",
    "            utils.validate_schema(input_output_dictionary)\n",
    "        except Exception as e:\n",
    "            print(\"Validation error:\", e)\n",
    "            return\n",
    "\n",
    "        #Create the controller\n",
    "        input_size = len(input_output_dictionary[\"input\"])\n",
    "        output_size = len(input_output_dictionary[\"output\"])\n",
    "\n",
    "        policy = PolicyNetwork(input_size, output_size, action_bound=1.0)\n",
    "\n",
    "        #Load the policy model\n",
    "        if policy_path is not None:\n",
    "            policy.load_state_dict(torch.load(policy_path))\n",
    "\n",
    "        neural_policy_controller = tb.NeuralPolicyControllerSystem(\n",
    "            input_size = input_size,\n",
    "            output_size = output_size,\n",
    "            input_output_schema = input_output_dictionary,\n",
    "            policy_model = policy,\n",
    "            id = \"neural_controller\"\n",
    "        )\n",
    "\n",
    "        #Find and remove the existing output connections and components\n",
    "        for output_component_key in input_output_dictionary[\"output\"]:\n",
    "            receiving_component = self.components[output_component_key]\n",
    "            found = False  \n",
    "            for connection_point in receiving_component.connectsAt:\n",
    "                if connection_point.inputPort == input_output_dictionary[\"output\"][output_component_key][\"signal_key\"]:\n",
    "                    #Remove the connection(s) to the receiving component\n",
    "                    for incoming_connection in connection_point.connectsSystemThrough:\n",
    "                        sender_component = incoming_connection.connectsSystem\n",
    "                        #If the sender component is not connected to any other component, remove it\n",
    "                        if len(sender_component.connectedThrough) == 1:\n",
    "                            self.remove_component(sender_component)\n",
    "                        else:\n",
    "                            self.remove_connection(sender_component, receiving_component, incoming_connection.outputPort, connection_point.inputPort)\n",
    "                    found = True \n",
    "                    break\n",
    "            if not found:\n",
    "                print(f\"Could not find connection for {output_component_key} and {input_output_dictionary['output'][output_component_key]['signal_key']}\")\n",
    "\n",
    "       \n",
    "        #Add the input connections\n",
    "        \n",
    "        for component_key in input_output_dictionary[\"input\"]:\n",
    "            try:\n",
    "                sender_component = self.components[component_key]\n",
    "            except KeyError:\n",
    "                print(f\"Could not find component {component_key}\")\n",
    "                continue\n",
    "            receiving_component = neural_policy_controller\n",
    "            self.add_connection(\n",
    "                sender_component,\n",
    "                receiving_component,\n",
    "                input_output_dictionary[\"input\"][component_key][\"signal_key\"],\n",
    "                \"actualValue\"\n",
    "            )\n",
    "\n",
    "\n",
    "        \n",
    "        # Define the output dictionary for the NeuralController using a dictionary comprehension\n",
    "        neural_policy_controller.output = {\n",
    "            f\"{component_key}_input_signal\": tps.Scalar()\n",
    "            for component_key in input_output_dictionary[\"output\"]\n",
    "        }\n",
    "\n",
    "        # Loop through the components and add connections\n",
    "        for component_key, output_info in input_output_dictionary[\"output\"].items():\n",
    "            output_key = f\"{component_key}_input_signal\"\n",
    "            receiver_component = self.components.get(component_key)\n",
    "            if receiver_component is None:\n",
    "                print(f\"Could not find component {component_key}\")\n",
    "                continue\n",
    "            self.add_connection(\n",
    "                neural_policy_controller,\n",
    "                receiver_component,\n",
    "                output_key,  # Use the unique output_key for each connection\n",
    "                output_info[\"signal_key\"]\n",
    "            )\n",
    "        # Define a custom initial dictionary for the NeuralController outputs:\n",
    "        custom_initial = {\"neural_controller\": {f\"{component_key}_input_signal\": tps.Scalar(0) for component_key in input_output_dictionary[\"output\"]}}\n",
    "        self.set_custom_initial_dict(custom_initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting model parameters\n",
    "Obtained from the parameter estimation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_parameters(self):\n",
    "    # Get component references\n",
    "    space = self.components[\"[020B][020B_space_heater]\"]\n",
    "    heating_controller = self.components[\"020B_temperature_heating_controller\"]\n",
    "    co2_controller = self.components[\"020B_co2_controller\"]\n",
    "    space_heater_valve = self.components[\"020B_space_heater_valve\"]\n",
    "    supply_damper = self.components[\"020B_room_supply_damper\"]\n",
    "    exhaust_damper = self.components[\"020B_room_exhaust_damper\"]\n",
    "\n",
    "    # Define parameters, components, and attributes\n",
    "    parameters = [\n",
    "        400,                     # CO2_start\n",
    "        0.35,                    # fraRad_sh\n",
    "        333.15,                  # T_a_nominal_sh\n",
    "        303.15,                  # T_b_nominal_sh\n",
    "        293.15,                  # TAir_nominal_sh\n",
    "        1570814.0716295305,      # C_wall\n",
    "        1301969.8981236944,      # C_air\n",
    "        25903.054795409433,      # C_boundary\n",
    "        0.036856517373434025,    # R_out\n",
    "        0.017648740304878827,    # R_in\n",
    "        0.0011821956470144748,   # R_boundary\n",
    "        0.12398026888438979,     # f_wall\n",
    "        0.507617122194916,       # f_air\n",
    "        277.24361247360997,      # Q_occ_gain\n",
    "        0.0010216031945522147,   # heating_controller kp\n",
    "        7.0307710027994075,      # heating_controller Ti\n",
    "        0.004852703638452023,    # co2_controller kp\n",
    "        8.44932909032329,        # co2_controller Ti\n",
    "        0.004898396001298374,    # m_flow_nominal\n",
    "        0.002977689536109641,    # dpFixed_nominal\n",
    "        21.025900835644723,      # T_boundary\n",
    "        6.790610786857144,       # supply_damper a\n",
    "        3.871299681159011,       # exhaust_damper a\n",
    "        0.060068382408930844,    # infiltration\n",
    "        3.1640648638260266e-05,  # CO2_occ_gain\n",
    "        436.36769116771666,      # Q_flow_nominal_sh\n",
    "        1.394916193442971,       # n_sh\n",
    "        378.45481913618784       # C_supply\n",
    "    ]\n",
    "\n",
    "    components = [\n",
    "        space, space, space, space, space,  # Initial space parameters\n",
    "        space, space, space, space, space, space, space, space, space,  # Space parameters\n",
    "        heating_controller, heating_controller,  # Heating controller parameters\n",
    "        co2_controller, co2_controller,  # CO2 controller parameters\n",
    "        space_heater_valve, space_heater_valve,  # Valve parameters\n",
    "        space,  # T_boundary\n",
    "        supply_damper, exhaust_damper,  # Damper parameters\n",
    "        space, space, space, space, space  # Remaining space parameters\n",
    "    ]\n",
    "\n",
    "    attributes = [\n",
    "        'CO2_start', 'fraRad_sh', 'T_a_nominal_sh', 'T_b_nominal_sh', 'TAir_nominal_sh',  # Initial space attributes\n",
    "        'C_wall', 'C_air', 'C_boundary', 'R_out', 'R_in', 'R_boundary', 'f_wall', 'f_air', 'Q_occ_gain',\n",
    "        'kp', 'Ti',  # Heating controller\n",
    "        'kp', 'Ti',  # CO2 controller\n",
    "        'm_flow_nominal', 'dpFixed_nominal',\n",
    "        'T_boundary',\n",
    "        'a', 'a',  # Dampers\n",
    "        'infiltration', 'CO2_occ_gain', 'Q_flow_nominal_sh', 'n_sh', 'C_supply'\n",
    "    ]\n",
    "\n",
    "    self.set_parameters_from_array(parameters, components, attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined function adding missing connections\n",
    "Also from the estimation parameter example, with the parameter setting and neural policy inserting functions now embedded into it.\n",
    "\n",
    "#### Here the policy is loaded\n",
    "As you can see in the last lines of the fcn method, the neural policy weights are loaded from an object in the example's directory called \"best_policy.pth\"\n",
    "These weights are the result of a standard Reinforcement Learning training process using PPO (Proximal Policy Optimization) see: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "\n",
    "It is out of the scope of the library to include the code on how to train Reinforcement Learning agents but there's plenty of very good resources on how to write a RL algorithm on the internet.\n",
    "\n",
    "Still, the general overview of the training process would be something like:\n",
    "1. Define the inputs/outputs for the RL agent, these should be already present in the model. A dictionary like \"policy_input_output.json\" in the example's directory contains the format for this information\n",
    "2. Create a tb.NeuralPolicyController object and insert it into the t4b model.\n",
    "3. Initialize the policy weights and an optimizer.\n",
    "4. Define a reward function, typically looking for a low energy consumption while keeping good indoor comfort levels\n",
    "5. Run a loop with a couple thousand simulations where each episode the Policy interacts with the model and get's feedback on how high of a score did it achieve\n",
    "6. Reinforce the actions that perform better (Obtain a higher reward). \n",
    "\n",
    "I recommend checking OpenAI's spinning up with Reinforcement Learning to learn about how to do this. https://spinningup.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn(self):\n",
    "    supply_water_schedule = tb.ScheduleSystem(\n",
    "    weekDayRulesetDict = { \n",
    "        \"ruleset_default_value\": 60,\n",
    "        \"ruleset_start_minute\": [],\n",
    "        \"ruleset_end_minute\": [],\n",
    "        \"ruleset_start_hour\": [],\n",
    "        \"ruleset_end_hour\": [],\n",
    "        \"ruleset_value\": []\n",
    "    },\n",
    "    id=\"supply_water_schedule\"\n",
    "    )\n",
    "    self.add_connection(supply_water_schedule, self.components[\"[020B][020B_space_heater]\"], \"scheduleValue\", \"supplyWaterTemperature\") # Add missing input\n",
    "    self.components[\"020B_temperature_sensor\"].filename = utils.get_path([\"parameter_estimation_example\", \"temperature_sensor.csv\"])\n",
    "    self.components[\"020B_co2_sensor\"].filename = utils.get_path([\"parameter_estimation_example\", \"co2_sensor.csv\"])\n",
    "    self.components[\"020B_valve_position_sensor\"].filename = utils.get_path([\"parameter_estimation_example\", \"valve_position_sensor.csv\"])\n",
    "    self.components[\"020B_damper_position_sensor\"].filename = utils.get_path([\"parameter_estimation_example\", \"damper_position_sensor.csv\"])\n",
    "    self.components[\"BTA004\"].filename = utils.get_path([\"parameter_estimation_example\", \"supply_air_temperature.csv\"])\n",
    "    self.components[\"020B_co2_setpoint\"].weekDayRulesetDict = {\"ruleset_default_value\": 900,\n",
    "                                                                    \"ruleset_start_minute\": [],\n",
    "                                                                    \"ruleset_end_minute\": [],\n",
    "                                                                    \"ruleset_start_hour\": [],\n",
    "                                                                    \"ruleset_end_hour\": [],\n",
    "                                                                    \"ruleset_value\": []}\n",
    "    self.components[\"020B_temperature_heating_setpoint\"].useSpreadsheet = True\n",
    "    self.components[\"020B_temperature_heating_setpoint\"].filename = utils.get_path([\"parameter_estimation_example\", \"temperature_heating_setpoint.csv\"])\n",
    "    self.components[\"outdoor_environment\"].filename = utils.get_path([\"parameter_estimation_example\", \"outdoor_environment.csv\"])\n",
    "    #Load the input/output dictionary from the file policy_input_output.json\n",
    "    with open(utils.get_path([\"neural_policy_controller_example\", \"policy_input_output.json\"])) as f:\n",
    "        input_output_dictionary = json.load(f)\n",
    "\n",
    "    policy_path = utils.get_path([\"neural_policy_controller_example\", \"best_policy.pth\"])\n",
    "\n",
    "    insert_neural_policy_in_fcn(self, input_output_dictionary, policy_path=policy_path)\n",
    "    set_model_parameters(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load(semantic_model_filename=filename, fcn=fcn, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Model (not required)\n",
    "We can visualize the model to see the components and connections.<br>\n",
    "This is not required, but it can be helpful for debugging and for building the model.<br>\n",
    "The nodes in the graph represent components, and the edges represent connections between components as defined earlier.\n",
    "Note that a neural_policy_controller component is present an has a higher number of inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "system_graph = os.path.join(model.graph_path, \"system_graph.png\")\n",
    "image = plt.imread(system_graph)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Run a simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 600  # Seconds\n",
    "start_time = datetime.datetime(year=2023, month=11, day=27, hour=0, minute=0, second=0,\n",
    "                                tzinfo=gettz(\"Europe/Copenhagen\"))\n",
    "end_time = datetime.datetime(year=2023, month=12, day=7, hour=0, minute=0, second=0,\n",
    "                            tzinfo=gettz(\"Europe/Copenhagen\"))\n",
    "\n",
    "simulator = tb.Simulator(model)\n",
    "simulator.simulate(model, start_time=start_time, end_time=end_time, step_size=step_size)\n",
    "print(\"Simulation completed successfully!\")\n",
    "\n",
    "# Plot the results using plot_component\n",
    "space_id = '[020B][020B_space_heater]'\n",
    "\n",
    "# Temperature plot\n",
    "plot.plot_component(\n",
    "    simulator,\n",
    "    components_1axis=[(space_id, 'indoorTemperature')],\n",
    "    components_2axis=[(space_id, 'outdoorTemperature')],\n",
    "    ylabel_1axis='Room Temperature [°C]',\n",
    "    ylabel_2axis='Outdoor Temperature [°C]',\n",
    "    show=True\n",
    ")\n",
    "\n",
    "# CO2 plot\n",
    "plot.plot_component(\n",
    "    simulator,\n",
    "    components_1axis=[(space_id, 'indoorCo2Concentration')],\n",
    "    components_2axis=[(space_id, 'airFlowRate')],\n",
    "    ylabel_1axis='CO2 Concentration [ppm]',\n",
    "    ylabel_2axis='Air Flow Rate [m³/s]',\n",
    "    show=True\n",
    ") \n",
    "\n",
    "\n",
    "# power plot\n",
    "plot.plot_component(\n",
    "    simulator,\n",
    "    components_1axis=[(space_id, 'spaceHeaterPower')],\n",
    "    ylabel_1axis='Space Heater Power [W]',\n",
    "    show=True\n",
    ")\n",
    "\n",
    "#plot the CO2 setpoint\n",
    "plot.plot_component(\n",
    "    simulator,\n",
    "    components_1axis=[(\"neural_controller\", '020B_co2_controller_input_signal')],\n",
    "    ylabel_1axis='CO2 Setpoint [ppm]',\n",
    "    show=True\n",
    ")\n",
    "\n",
    "#plot the temperature setpoint\n",
    "plot.plot_component(\n",
    "    simulator,\n",
    "    components_1axis=[(\"neural_controller\", '020B_temperature_heating_controller_input_signal')],\n",
    "    ylabel_1axis='Temperature Setpoint [°C]',\n",
    "    show=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on the results:\n",
    "\n",
    "The results from using the neural policy can be compared to the baseline model simulation obtained from the parameter_estimation_example. As it can be appreciated the policy decides to almost completely turn off the space heater as it finds that the isolation in this specific model is good enough to keep the indoor temperature at around 20.5 degrees during all the simulation time.\n",
    "\n",
    "Since the occupancy levels are also not major, the policy decided to keep a CO2 setpoint at around 3000 ppm. While keeping the indoor CO2 concentration at around 430 ppm.\n",
    "\n",
    "These results are not completely realistic since the core model is a very simple one. However this also serves as a good example of the typical behavior of RL agents, they tend to exploit singularities of the model they're trained on. This highlights the importance of having a robust and realistic model when training RL agents with it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t4b_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
